# Tokyo-Olympics-Azure-Data-Engineering 

## Description
This project aims to demonstrate the process of data extraction, transformation and load(ETL) and analysis using various Azure services. The dataset used in this project was downloaded from Kaggle.com and uploaded to GitHub for easy access.

### Steps Involved

 1-   Data Collection: The dataset was obtained from Kaggle.com and saved it on my github to be accessable by me.
 
 2-   Environment Setup: Data Factory Services on Azure was utilized to extract the raw dataset via HTTP connection. The raw data was stored in Azure Data Lake Services in a container storage as a file called raw_data.
 
 3-   Data Transformation: Linked the data from Azure Data Lake to Databricks service. Applied necessary transformations in Databricks, such as changing data types and viewing values. Loaded the transformed data back into Azure Data Lake.
 
 4-   Data Analysis: Utilized** Synapse Analytics** to create tables and write SQL scripts for data analysis.
 
 5-   Future Steps: Integrate the data with **Tableau** for further insights and visualization.
 

 ### Technologies Used
 

  1-  **Azure Data Factory Services**
  
  2-  **Azure Data Lake Services**
  
  3-  **Azure Databricks**
  
  4-  **Azure Synapse Analytics**
  
  5-  **Tableau** 


### Usage

   1- Clone the repository to your local machine.
   
   2- Follow the instructions in each service's documentation to set up the environment and execute the necessary steps.
   
   3- Explore the data analysis and insights provided.





  
